{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 885,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1702127659574468,
      "grad_norm": 3.6330208778381348,
      "learning_rate": 4.9491525423728815e-05,
      "loss": 8.8142,
      "step": 10
    },
    {
      "epoch": 0.3404255319148936,
      "grad_norm": 4.392602443695068,
      "learning_rate": 4.892655367231639e-05,
      "loss": 7.7487,
      "step": 20
    },
    {
      "epoch": 0.5106382978723404,
      "grad_norm": 3.3592076301574707,
      "learning_rate": 4.836158192090396e-05,
      "loss": 7.325,
      "step": 30
    },
    {
      "epoch": 0.6808510638297872,
      "grad_norm": 2.414419651031494,
      "learning_rate": 4.7796610169491526e-05,
      "loss": 6.7846,
      "step": 40
    },
    {
      "epoch": 0.851063829787234,
      "grad_norm": 3.1877121925354004,
      "learning_rate": 4.72316384180791e-05,
      "loss": 5.8284,
      "step": 50
    },
    {
      "epoch": 1.0170212765957447,
      "grad_norm": 1.3415966033935547,
      "learning_rate": 4.666666666666667e-05,
      "loss": 5.3432,
      "step": 60
    },
    {
      "epoch": 1.1872340425531915,
      "grad_norm": 1.799818992614746,
      "learning_rate": 4.610169491525424e-05,
      "loss": 4.9247,
      "step": 70
    },
    {
      "epoch": 1.3574468085106384,
      "grad_norm": 1.1730061769485474,
      "learning_rate": 4.553672316384181e-05,
      "loss": 4.7421,
      "step": 80
    },
    {
      "epoch": 1.527659574468085,
      "grad_norm": 1.5055874586105347,
      "learning_rate": 4.497175141242938e-05,
      "loss": 4.638,
      "step": 90
    },
    {
      "epoch": 1.697872340425532,
      "grad_norm": 2.077824592590332,
      "learning_rate": 4.440677966101695e-05,
      "loss": 4.5412,
      "step": 100
    },
    {
      "epoch": 1.8680851063829786,
      "grad_norm": 1.8773763179779053,
      "learning_rate": 4.384180790960452e-05,
      "loss": 4.3884,
      "step": 110
    },
    {
      "epoch": 2.0340425531914894,
      "grad_norm": 1.5715436935424805,
      "learning_rate": 4.3276836158192094e-05,
      "loss": 4.4316,
      "step": 120
    },
    {
      "epoch": 2.204255319148936,
      "grad_norm": 1.42667555809021,
      "learning_rate": 4.271186440677966e-05,
      "loss": 4.2167,
      "step": 130
    },
    {
      "epoch": 2.374468085106383,
      "grad_norm": 2.0798838138580322,
      "learning_rate": 4.214689265536723e-05,
      "loss": 4.0602,
      "step": 140
    },
    {
      "epoch": 2.5446808510638297,
      "grad_norm": 1.4210158586502075,
      "learning_rate": 4.1581920903954805e-05,
      "loss": 4.063,
      "step": 150
    },
    {
      "epoch": 2.7148936170212767,
      "grad_norm": 1.5160611867904663,
      "learning_rate": 4.101694915254237e-05,
      "loss": 3.9774,
      "step": 160
    },
    {
      "epoch": 2.8851063829787233,
      "grad_norm": 1.7363053560256958,
      "learning_rate": 4.045197740112995e-05,
      "loss": 4.0547,
      "step": 170
    },
    {
      "epoch": 3.051063829787234,
      "grad_norm": 1.4708187580108643,
      "learning_rate": 3.9887005649717516e-05,
      "loss": 3.9114,
      "step": 180
    },
    {
      "epoch": 3.2212765957446807,
      "grad_norm": 1.9271079301834106,
      "learning_rate": 3.932203389830509e-05,
      "loss": 3.8246,
      "step": 190
    },
    {
      "epoch": 3.391489361702128,
      "grad_norm": 2.325526714324951,
      "learning_rate": 3.875706214689266e-05,
      "loss": 3.8688,
      "step": 200
    },
    {
      "epoch": 3.5617021276595744,
      "grad_norm": 1.8999170064926147,
      "learning_rate": 3.819209039548023e-05,
      "loss": 3.7081,
      "step": 210
    },
    {
      "epoch": 3.731914893617021,
      "grad_norm": 1.7591350078582764,
      "learning_rate": 3.76271186440678e-05,
      "loss": 3.7337,
      "step": 220
    },
    {
      "epoch": 3.902127659574468,
      "grad_norm": 2.091078519821167,
      "learning_rate": 3.7062146892655366e-05,
      "loss": 3.6992,
      "step": 230
    },
    {
      "epoch": 4.068085106382979,
      "grad_norm": 2.0971696376800537,
      "learning_rate": 3.649717514124294e-05,
      "loss": 3.6007,
      "step": 240
    },
    {
      "epoch": 4.238297872340426,
      "grad_norm": 2.2576828002929688,
      "learning_rate": 3.593220338983051e-05,
      "loss": 3.4975,
      "step": 250
    },
    {
      "epoch": 4.408510638297872,
      "grad_norm": 1.9867867231369019,
      "learning_rate": 3.536723163841808e-05,
      "loss": 3.5405,
      "step": 260
    },
    {
      "epoch": 4.578723404255319,
      "grad_norm": 2.091723680496216,
      "learning_rate": 3.480225988700565e-05,
      "loss": 3.4723,
      "step": 270
    },
    {
      "epoch": 4.748936170212766,
      "grad_norm": 2.1565604209899902,
      "learning_rate": 3.423728813559322e-05,
      "loss": 3.4556,
      "step": 280
    },
    {
      "epoch": 4.919148936170213,
      "grad_norm": 2.4759774208068848,
      "learning_rate": 3.367231638418079e-05,
      "loss": 3.4376,
      "step": 290
    },
    {
      "epoch": 5.085106382978723,
      "grad_norm": 2.385643720626831,
      "learning_rate": 3.310734463276836e-05,
      "loss": 3.3228,
      "step": 300
    },
    {
      "epoch": 5.25531914893617,
      "grad_norm": 3.5415337085723877,
      "learning_rate": 3.2542372881355934e-05,
      "loss": 3.2074,
      "step": 310
    },
    {
      "epoch": 5.425531914893617,
      "grad_norm": 2.356210708618164,
      "learning_rate": 3.19774011299435e-05,
      "loss": 3.153,
      "step": 320
    },
    {
      "epoch": 5.595744680851064,
      "grad_norm": 2.4332401752471924,
      "learning_rate": 3.141242937853108e-05,
      "loss": 3.2076,
      "step": 330
    },
    {
      "epoch": 5.76595744680851,
      "grad_norm": 2.6413369178771973,
      "learning_rate": 3.0847457627118645e-05,
      "loss": 3.1972,
      "step": 340
    },
    {
      "epoch": 5.9361702127659575,
      "grad_norm": 2.659703493118286,
      "learning_rate": 3.0282485875706218e-05,
      "loss": 3.2292,
      "step": 350
    },
    {
      "epoch": 6.102127659574468,
      "grad_norm": 2.6559712886810303,
      "learning_rate": 2.9717514124293787e-05,
      "loss": 3.0624,
      "step": 360
    },
    {
      "epoch": 6.272340425531915,
      "grad_norm": 2.476083278656006,
      "learning_rate": 2.915254237288136e-05,
      "loss": 2.9804,
      "step": 370
    },
    {
      "epoch": 6.4425531914893615,
      "grad_norm": 2.673874855041504,
      "learning_rate": 2.858757062146893e-05,
      "loss": 2.9578,
      "step": 380
    },
    {
      "epoch": 6.6127659574468085,
      "grad_norm": 2.9816174507141113,
      "learning_rate": 2.8022598870056498e-05,
      "loss": 2.9051,
      "step": 390
    },
    {
      "epoch": 6.782978723404256,
      "grad_norm": 2.7318131923675537,
      "learning_rate": 2.7457627118644068e-05,
      "loss": 2.9111,
      "step": 400
    },
    {
      "epoch": 6.953191489361702,
      "grad_norm": 2.894453525543213,
      "learning_rate": 2.689265536723164e-05,
      "loss": 2.9669,
      "step": 410
    },
    {
      "epoch": 7.1191489361702125,
      "grad_norm": 2.7531566619873047,
      "learning_rate": 2.632768361581921e-05,
      "loss": 2.7942,
      "step": 420
    },
    {
      "epoch": 7.2893617021276595,
      "grad_norm": 2.8273000717163086,
      "learning_rate": 2.576271186440678e-05,
      "loss": 2.6928,
      "step": 430
    },
    {
      "epoch": 7.459574468085107,
      "grad_norm": 2.786475658416748,
      "learning_rate": 2.519774011299435e-05,
      "loss": 2.7638,
      "step": 440
    },
    {
      "epoch": 7.629787234042553,
      "grad_norm": 2.943845510482788,
      "learning_rate": 2.4632768361581924e-05,
      "loss": 2.699,
      "step": 450
    },
    {
      "epoch": 7.8,
      "grad_norm": 2.680083751678467,
      "learning_rate": 2.4067796610169493e-05,
      "loss": 2.7058,
      "step": 460
    },
    {
      "epoch": 7.970212765957447,
      "grad_norm": 3.3740391731262207,
      "learning_rate": 2.3502824858757063e-05,
      "loss": 2.7127,
      "step": 470
    },
    {
      "epoch": 8.136170212765958,
      "grad_norm": 2.9452245235443115,
      "learning_rate": 2.2937853107344635e-05,
      "loss": 2.5956,
      "step": 480
    },
    {
      "epoch": 8.306382978723404,
      "grad_norm": 3.0366671085357666,
      "learning_rate": 2.2372881355932205e-05,
      "loss": 2.4392,
      "step": 490
    },
    {
      "epoch": 8.476595744680852,
      "grad_norm": 2.8986010551452637,
      "learning_rate": 2.1807909604519774e-05,
      "loss": 2.5005,
      "step": 500
    },
    {
      "epoch": 8.646808510638298,
      "grad_norm": 3.1678597927093506,
      "learning_rate": 2.1242937853107343e-05,
      "loss": 2.4951,
      "step": 510
    },
    {
      "epoch": 8.817021276595744,
      "grad_norm": 3.056101083755493,
      "learning_rate": 2.0677966101694916e-05,
      "loss": 2.5028,
      "step": 520
    },
    {
      "epoch": 8.987234042553192,
      "grad_norm": 3.174797534942627,
      "learning_rate": 2.011299435028249e-05,
      "loss": 2.514,
      "step": 530
    },
    {
      "epoch": 9.153191489361703,
      "grad_norm": 3.2382752895355225,
      "learning_rate": 1.9548022598870058e-05,
      "loss": 2.3607,
      "step": 540
    },
    {
      "epoch": 9.323404255319149,
      "grad_norm": 3.0618369579315186,
      "learning_rate": 1.8983050847457627e-05,
      "loss": 2.3244,
      "step": 550
    },
    {
      "epoch": 9.493617021276595,
      "grad_norm": 3.068930149078369,
      "learning_rate": 1.84180790960452e-05,
      "loss": 2.2955,
      "step": 560
    },
    {
      "epoch": 9.663829787234043,
      "grad_norm": 3.599112033843994,
      "learning_rate": 1.785310734463277e-05,
      "loss": 2.3379,
      "step": 570
    },
    {
      "epoch": 9.83404255319149,
      "grad_norm": 3.001988649368286,
      "learning_rate": 1.7288135593220338e-05,
      "loss": 2.304,
      "step": 580
    },
    {
      "epoch": 10.0,
      "grad_norm": 4.475876808166504,
      "learning_rate": 1.672316384180791e-05,
      "loss": 2.2806,
      "step": 590
    },
    {
      "epoch": 10.170212765957446,
      "grad_norm": 2.659186601638794,
      "learning_rate": 1.615819209039548e-05,
      "loss": 2.2036,
      "step": 600
    },
    {
      "epoch": 10.340425531914894,
      "grad_norm": 2.832388401031494,
      "learning_rate": 1.5593220338983053e-05,
      "loss": 2.1583,
      "step": 610
    },
    {
      "epoch": 10.51063829787234,
      "grad_norm": 3.628649950027466,
      "learning_rate": 1.5028248587570624e-05,
      "loss": 2.1214,
      "step": 620
    },
    {
      "epoch": 10.680851063829786,
      "grad_norm": 2.8919849395751953,
      "learning_rate": 1.4463276836158193e-05,
      "loss": 2.1387,
      "step": 630
    },
    {
      "epoch": 10.851063829787234,
      "grad_norm": 3.257025718688965,
      "learning_rate": 1.3898305084745764e-05,
      "loss": 2.1309,
      "step": 640
    },
    {
      "epoch": 11.017021276595745,
      "grad_norm": 2.9637556076049805,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 2.1334,
      "step": 650
    },
    {
      "epoch": 11.187234042553191,
      "grad_norm": 2.848738193511963,
      "learning_rate": 1.2768361581920904e-05,
      "loss": 2.0087,
      "step": 660
    },
    {
      "epoch": 11.357446808510637,
      "grad_norm": 2.6733527183532715,
      "learning_rate": 1.2203389830508475e-05,
      "loss": 1.9935,
      "step": 670
    },
    {
      "epoch": 11.527659574468085,
      "grad_norm": 3.0700132846832275,
      "learning_rate": 1.1638418079096046e-05,
      "loss": 1.9474,
      "step": 680
    },
    {
      "epoch": 11.697872340425532,
      "grad_norm": 2.814941167831421,
      "learning_rate": 1.1073446327683615e-05,
      "loss": 2.0329,
      "step": 690
    },
    {
      "epoch": 11.86808510638298,
      "grad_norm": 2.8067455291748047,
      "learning_rate": 1.0508474576271186e-05,
      "loss": 2.0177,
      "step": 700
    },
    {
      "epoch": 12.034042553191489,
      "grad_norm": 2.8323347568511963,
      "learning_rate": 9.943502824858757e-06,
      "loss": 2.0159,
      "step": 710
    },
    {
      "epoch": 12.204255319148936,
      "grad_norm": 2.660975933074951,
      "learning_rate": 9.378531073446328e-06,
      "loss": 1.9269,
      "step": 720
    },
    {
      "epoch": 12.374468085106383,
      "grad_norm": 2.586393117904663,
      "learning_rate": 8.8135593220339e-06,
      "loss": 1.8725,
      "step": 730
    },
    {
      "epoch": 12.54468085106383,
      "grad_norm": 2.5242276191711426,
      "learning_rate": 8.248587570621469e-06,
      "loss": 1.9315,
      "step": 740
    },
    {
      "epoch": 12.714893617021277,
      "grad_norm": 2.631805896759033,
      "learning_rate": 7.683615819209041e-06,
      "loss": 1.8404,
      "step": 750
    },
    {
      "epoch": 12.885106382978723,
      "grad_norm": 2.7336556911468506,
      "learning_rate": 7.1186440677966106e-06,
      "loss": 1.8532,
      "step": 760
    },
    {
      "epoch": 13.051063829787234,
      "grad_norm": 2.642752170562744,
      "learning_rate": 6.553672316384181e-06,
      "loss": 1.8431,
      "step": 770
    },
    {
      "epoch": 13.221276595744682,
      "grad_norm": 2.636612892150879,
      "learning_rate": 5.988700564971752e-06,
      "loss": 1.7683,
      "step": 780
    },
    {
      "epoch": 13.391489361702128,
      "grad_norm": 2.4817066192626953,
      "learning_rate": 5.423728813559322e-06,
      "loss": 1.7705,
      "step": 790
    },
    {
      "epoch": 13.561702127659574,
      "grad_norm": 2.4663589000701904,
      "learning_rate": 4.858757062146893e-06,
      "loss": 1.8209,
      "step": 800
    },
    {
      "epoch": 13.731914893617022,
      "grad_norm": 2.565319538116455,
      "learning_rate": 4.293785310734464e-06,
      "loss": 1.7953,
      "step": 810
    },
    {
      "epoch": 13.902127659574468,
      "grad_norm": 2.6706180572509766,
      "learning_rate": 3.7288135593220342e-06,
      "loss": 1.8255,
      "step": 820
    },
    {
      "epoch": 14.068085106382979,
      "grad_norm": 2.4186630249023438,
      "learning_rate": 3.1638418079096044e-06,
      "loss": 1.7891,
      "step": 830
    },
    {
      "epoch": 14.238297872340425,
      "grad_norm": 2.4479193687438965,
      "learning_rate": 2.5988700564971753e-06,
      "loss": 1.7017,
      "step": 840
    },
    {
      "epoch": 14.408510638297873,
      "grad_norm": 2.6266796588897705,
      "learning_rate": 2.033898305084746e-06,
      "loss": 1.7649,
      "step": 850
    },
    {
      "epoch": 14.578723404255319,
      "grad_norm": 2.4670772552490234,
      "learning_rate": 1.4689265536723164e-06,
      "loss": 1.7572,
      "step": 860
    },
    {
      "epoch": 14.748936170212765,
      "grad_norm": 2.403550148010254,
      "learning_rate": 9.039548022598871e-07,
      "loss": 1.7742,
      "step": 870
    },
    {
      "epoch": 14.919148936170213,
      "grad_norm": 2.3366665840148926,
      "learning_rate": 3.389830508474576e-07,
      "loss": 1.7186,
      "step": 880
    }
  ],
  "logging_steps": 10,
  "max_steps": 885,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1050772157890560.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
