{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.476595744680852,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1702127659574468,
      "grad_norm": 3.6330208778381348,
      "learning_rate": 4.9491525423728815e-05,
      "loss": 8.8142,
      "step": 10
    },
    {
      "epoch": 0.3404255319148936,
      "grad_norm": 4.392602443695068,
      "learning_rate": 4.892655367231639e-05,
      "loss": 7.7487,
      "step": 20
    },
    {
      "epoch": 0.5106382978723404,
      "grad_norm": 3.3592076301574707,
      "learning_rate": 4.836158192090396e-05,
      "loss": 7.325,
      "step": 30
    },
    {
      "epoch": 0.6808510638297872,
      "grad_norm": 2.414419651031494,
      "learning_rate": 4.7796610169491526e-05,
      "loss": 6.7846,
      "step": 40
    },
    {
      "epoch": 0.851063829787234,
      "grad_norm": 3.1877121925354004,
      "learning_rate": 4.72316384180791e-05,
      "loss": 5.8284,
      "step": 50
    },
    {
      "epoch": 1.0170212765957447,
      "grad_norm": 1.3415966033935547,
      "learning_rate": 4.666666666666667e-05,
      "loss": 5.3432,
      "step": 60
    },
    {
      "epoch": 1.1872340425531915,
      "grad_norm": 1.799818992614746,
      "learning_rate": 4.610169491525424e-05,
      "loss": 4.9247,
      "step": 70
    },
    {
      "epoch": 1.3574468085106384,
      "grad_norm": 1.1730061769485474,
      "learning_rate": 4.553672316384181e-05,
      "loss": 4.7421,
      "step": 80
    },
    {
      "epoch": 1.527659574468085,
      "grad_norm": 1.5055874586105347,
      "learning_rate": 4.497175141242938e-05,
      "loss": 4.638,
      "step": 90
    },
    {
      "epoch": 1.697872340425532,
      "grad_norm": 2.077824592590332,
      "learning_rate": 4.440677966101695e-05,
      "loss": 4.5412,
      "step": 100
    },
    {
      "epoch": 1.8680851063829786,
      "grad_norm": 1.8773763179779053,
      "learning_rate": 4.384180790960452e-05,
      "loss": 4.3884,
      "step": 110
    },
    {
      "epoch": 2.0340425531914894,
      "grad_norm": 1.5715436935424805,
      "learning_rate": 4.3276836158192094e-05,
      "loss": 4.4316,
      "step": 120
    },
    {
      "epoch": 2.204255319148936,
      "grad_norm": 1.42667555809021,
      "learning_rate": 4.271186440677966e-05,
      "loss": 4.2167,
      "step": 130
    },
    {
      "epoch": 2.374468085106383,
      "grad_norm": 2.0798838138580322,
      "learning_rate": 4.214689265536723e-05,
      "loss": 4.0602,
      "step": 140
    },
    {
      "epoch": 2.5446808510638297,
      "grad_norm": 1.4210158586502075,
      "learning_rate": 4.1581920903954805e-05,
      "loss": 4.063,
      "step": 150
    },
    {
      "epoch": 2.7148936170212767,
      "grad_norm": 1.5160611867904663,
      "learning_rate": 4.101694915254237e-05,
      "loss": 3.9774,
      "step": 160
    },
    {
      "epoch": 2.8851063829787233,
      "grad_norm": 1.7363053560256958,
      "learning_rate": 4.045197740112995e-05,
      "loss": 4.0547,
      "step": 170
    },
    {
      "epoch": 3.051063829787234,
      "grad_norm": 1.4708187580108643,
      "learning_rate": 3.9887005649717516e-05,
      "loss": 3.9114,
      "step": 180
    },
    {
      "epoch": 3.2212765957446807,
      "grad_norm": 1.9271079301834106,
      "learning_rate": 3.932203389830509e-05,
      "loss": 3.8246,
      "step": 190
    },
    {
      "epoch": 3.391489361702128,
      "grad_norm": 2.325526714324951,
      "learning_rate": 3.875706214689266e-05,
      "loss": 3.8688,
      "step": 200
    },
    {
      "epoch": 3.5617021276595744,
      "grad_norm": 1.8999170064926147,
      "learning_rate": 3.819209039548023e-05,
      "loss": 3.7081,
      "step": 210
    },
    {
      "epoch": 3.731914893617021,
      "grad_norm": 1.7591350078582764,
      "learning_rate": 3.76271186440678e-05,
      "loss": 3.7337,
      "step": 220
    },
    {
      "epoch": 3.902127659574468,
      "grad_norm": 2.091078519821167,
      "learning_rate": 3.7062146892655366e-05,
      "loss": 3.6992,
      "step": 230
    },
    {
      "epoch": 4.068085106382979,
      "grad_norm": 2.0971696376800537,
      "learning_rate": 3.649717514124294e-05,
      "loss": 3.6007,
      "step": 240
    },
    {
      "epoch": 4.238297872340426,
      "grad_norm": 2.2576828002929688,
      "learning_rate": 3.593220338983051e-05,
      "loss": 3.4975,
      "step": 250
    },
    {
      "epoch": 4.408510638297872,
      "grad_norm": 1.9867867231369019,
      "learning_rate": 3.536723163841808e-05,
      "loss": 3.5405,
      "step": 260
    },
    {
      "epoch": 4.578723404255319,
      "grad_norm": 2.091723680496216,
      "learning_rate": 3.480225988700565e-05,
      "loss": 3.4723,
      "step": 270
    },
    {
      "epoch": 4.748936170212766,
      "grad_norm": 2.1565604209899902,
      "learning_rate": 3.423728813559322e-05,
      "loss": 3.4556,
      "step": 280
    },
    {
      "epoch": 4.919148936170213,
      "grad_norm": 2.4759774208068848,
      "learning_rate": 3.367231638418079e-05,
      "loss": 3.4376,
      "step": 290
    },
    {
      "epoch": 5.085106382978723,
      "grad_norm": 2.385643720626831,
      "learning_rate": 3.310734463276836e-05,
      "loss": 3.3228,
      "step": 300
    },
    {
      "epoch": 5.25531914893617,
      "grad_norm": 3.5415337085723877,
      "learning_rate": 3.2542372881355934e-05,
      "loss": 3.2074,
      "step": 310
    },
    {
      "epoch": 5.425531914893617,
      "grad_norm": 2.356210708618164,
      "learning_rate": 3.19774011299435e-05,
      "loss": 3.153,
      "step": 320
    },
    {
      "epoch": 5.595744680851064,
      "grad_norm": 2.4332401752471924,
      "learning_rate": 3.141242937853108e-05,
      "loss": 3.2076,
      "step": 330
    },
    {
      "epoch": 5.76595744680851,
      "grad_norm": 2.6413369178771973,
      "learning_rate": 3.0847457627118645e-05,
      "loss": 3.1972,
      "step": 340
    },
    {
      "epoch": 5.9361702127659575,
      "grad_norm": 2.659703493118286,
      "learning_rate": 3.0282485875706218e-05,
      "loss": 3.2292,
      "step": 350
    },
    {
      "epoch": 6.102127659574468,
      "grad_norm": 2.6559712886810303,
      "learning_rate": 2.9717514124293787e-05,
      "loss": 3.0624,
      "step": 360
    },
    {
      "epoch": 6.272340425531915,
      "grad_norm": 2.476083278656006,
      "learning_rate": 2.915254237288136e-05,
      "loss": 2.9804,
      "step": 370
    },
    {
      "epoch": 6.4425531914893615,
      "grad_norm": 2.673874855041504,
      "learning_rate": 2.858757062146893e-05,
      "loss": 2.9578,
      "step": 380
    },
    {
      "epoch": 6.6127659574468085,
      "grad_norm": 2.9816174507141113,
      "learning_rate": 2.8022598870056498e-05,
      "loss": 2.9051,
      "step": 390
    },
    {
      "epoch": 6.782978723404256,
      "grad_norm": 2.7318131923675537,
      "learning_rate": 2.7457627118644068e-05,
      "loss": 2.9111,
      "step": 400
    },
    {
      "epoch": 6.953191489361702,
      "grad_norm": 2.894453525543213,
      "learning_rate": 2.689265536723164e-05,
      "loss": 2.9669,
      "step": 410
    },
    {
      "epoch": 7.1191489361702125,
      "grad_norm": 2.7531566619873047,
      "learning_rate": 2.632768361581921e-05,
      "loss": 2.7942,
      "step": 420
    },
    {
      "epoch": 7.2893617021276595,
      "grad_norm": 2.8273000717163086,
      "learning_rate": 2.576271186440678e-05,
      "loss": 2.6928,
      "step": 430
    },
    {
      "epoch": 7.459574468085107,
      "grad_norm": 2.786475658416748,
      "learning_rate": 2.519774011299435e-05,
      "loss": 2.7638,
      "step": 440
    },
    {
      "epoch": 7.629787234042553,
      "grad_norm": 2.943845510482788,
      "learning_rate": 2.4632768361581924e-05,
      "loss": 2.699,
      "step": 450
    },
    {
      "epoch": 7.8,
      "grad_norm": 2.680083751678467,
      "learning_rate": 2.4067796610169493e-05,
      "loss": 2.7058,
      "step": 460
    },
    {
      "epoch": 7.970212765957447,
      "grad_norm": 3.3740391731262207,
      "learning_rate": 2.3502824858757063e-05,
      "loss": 2.7127,
      "step": 470
    },
    {
      "epoch": 8.136170212765958,
      "grad_norm": 2.9452245235443115,
      "learning_rate": 2.2937853107344635e-05,
      "loss": 2.5956,
      "step": 480
    },
    {
      "epoch": 8.306382978723404,
      "grad_norm": 3.0366671085357666,
      "learning_rate": 2.2372881355932205e-05,
      "loss": 2.4392,
      "step": 490
    },
    {
      "epoch": 8.476595744680852,
      "grad_norm": 2.8986010551452637,
      "learning_rate": 2.1807909604519774e-05,
      "loss": 2.5005,
      "step": 500
    }
  ],
  "logging_steps": 10,
  "max_steps": 885,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 593869239484416.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
