{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 1535,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.032599837000814993,
      "grad_norm": 2.9511754512786865,
      "learning_rate": 4.970684039087948e-05,
      "loss": 9.1548,
      "step": 10
    },
    {
      "epoch": 0.06519967400162999,
      "grad_norm": 2.325695753097534,
      "learning_rate": 4.9381107491856684e-05,
      "loss": 8.3082,
      "step": 20
    },
    {
      "epoch": 0.097799511002445,
      "grad_norm": 2.1370835304260254,
      "learning_rate": 4.905537459283388e-05,
      "loss": 7.8922,
      "step": 30
    },
    {
      "epoch": 0.13039934800325997,
      "grad_norm": 1.6312851905822754,
      "learning_rate": 4.872964169381108e-05,
      "loss": 7.6874,
      "step": 40
    },
    {
      "epoch": 0.16299918500407498,
      "grad_norm": 1.3342680931091309,
      "learning_rate": 4.8403908794788275e-05,
      "loss": 7.3181,
      "step": 50
    },
    {
      "epoch": 0.19559902200489,
      "grad_norm": 1.8226497173309326,
      "learning_rate": 4.807817589576548e-05,
      "loss": 6.8665,
      "step": 60
    },
    {
      "epoch": 0.22819885900570497,
      "grad_norm": 1.5065134763717651,
      "learning_rate": 4.7752442996742675e-05,
      "loss": 6.322,
      "step": 70
    },
    {
      "epoch": 0.26079869600651995,
      "grad_norm": 1.1547986268997192,
      "learning_rate": 4.742671009771987e-05,
      "loss": 6.143,
      "step": 80
    },
    {
      "epoch": 0.293398533007335,
      "grad_norm": 1.2227290868759155,
      "learning_rate": 4.710097719869707e-05,
      "loss": 6.1509,
      "step": 90
    },
    {
      "epoch": 0.32599837000814996,
      "grad_norm": 1.0631858110427856,
      "learning_rate": 4.677524429967427e-05,
      "loss": 6.1596,
      "step": 100
    },
    {
      "epoch": 0.35859820700896494,
      "grad_norm": 1.2084180116653442,
      "learning_rate": 4.644951140065147e-05,
      "loss": 5.9411,
      "step": 110
    },
    {
      "epoch": 0.39119804400978,
      "grad_norm": 1.3386465311050415,
      "learning_rate": 4.6123778501628666e-05,
      "loss": 5.9924,
      "step": 120
    },
    {
      "epoch": 0.42379788101059496,
      "grad_norm": 1.1561083793640137,
      "learning_rate": 4.579804560260586e-05,
      "loss": 5.7753,
      "step": 130
    },
    {
      "epoch": 0.45639771801140994,
      "grad_norm": 1.2314393520355225,
      "learning_rate": 4.547231270358307e-05,
      "loss": 5.8435,
      "step": 140
    },
    {
      "epoch": 0.4889975550122249,
      "grad_norm": 1.39322829246521,
      "learning_rate": 4.5146579804560264e-05,
      "loss": 5.8537,
      "step": 150
    },
    {
      "epoch": 0.5215973920130399,
      "grad_norm": 1.2822579145431519,
      "learning_rate": 4.482084690553746e-05,
      "loss": 5.7398,
      "step": 160
    },
    {
      "epoch": 0.5541972290138549,
      "grad_norm": 1.2061846256256104,
      "learning_rate": 4.449511400651466e-05,
      "loss": 5.5672,
      "step": 170
    },
    {
      "epoch": 0.58679706601467,
      "grad_norm": 1.4082894325256348,
      "learning_rate": 4.416938110749186e-05,
      "loss": 5.7025,
      "step": 180
    },
    {
      "epoch": 0.6193969030154849,
      "grad_norm": 1.3852851390838623,
      "learning_rate": 4.384364820846906e-05,
      "loss": 5.6462,
      "step": 190
    },
    {
      "epoch": 0.6519967400162999,
      "grad_norm": 1.2222554683685303,
      "learning_rate": 4.3517915309446255e-05,
      "loss": 5.8401,
      "step": 200
    },
    {
      "epoch": 0.684596577017115,
      "grad_norm": 1.1123111248016357,
      "learning_rate": 4.319218241042345e-05,
      "loss": 5.5465,
      "step": 210
    },
    {
      "epoch": 0.7171964140179299,
      "grad_norm": 1.5075178146362305,
      "learning_rate": 4.2866449511400656e-05,
      "loss": 5.4998,
      "step": 220
    },
    {
      "epoch": 0.7497962510187449,
      "grad_norm": 1.6821863651275635,
      "learning_rate": 4.254071661237785e-05,
      "loss": 5.478,
      "step": 230
    },
    {
      "epoch": 0.78239608801956,
      "grad_norm": 1.2902334928512573,
      "learning_rate": 4.221498371335505e-05,
      "loss": 5.5179,
      "step": 240
    },
    {
      "epoch": 0.8149959250203749,
      "grad_norm": 1.2116942405700684,
      "learning_rate": 4.1889250814332246e-05,
      "loss": 5.6542,
      "step": 250
    },
    {
      "epoch": 0.8475957620211899,
      "grad_norm": 1.459987998008728,
      "learning_rate": 4.156351791530945e-05,
      "loss": 5.6515,
      "step": 260
    },
    {
      "epoch": 0.8801955990220048,
      "grad_norm": 1.308855652809143,
      "learning_rate": 4.123778501628665e-05,
      "loss": 5.5287,
      "step": 270
    },
    {
      "epoch": 0.9127954360228199,
      "grad_norm": 1.5057082176208496,
      "learning_rate": 4.0912052117263844e-05,
      "loss": 5.5111,
      "step": 280
    },
    {
      "epoch": 0.9453952730236349,
      "grad_norm": 1.5041412115097046,
      "learning_rate": 4.058631921824104e-05,
      "loss": 5.4791,
      "step": 290
    },
    {
      "epoch": 0.9779951100244498,
      "grad_norm": 1.337431788444519,
      "learning_rate": 4.026058631921824e-05,
      "loss": 5.4443,
      "step": 300
    },
    {
      "epoch": 1.0097799511002445,
      "grad_norm": 1.5179822444915771,
      "learning_rate": 3.993485342019544e-05,
      "loss": 5.3149,
      "step": 310
    },
    {
      "epoch": 1.0423797881010595,
      "grad_norm": 1.3684002161026,
      "learning_rate": 3.960912052117264e-05,
      "loss": 5.2452,
      "step": 320
    },
    {
      "epoch": 1.0749796251018744,
      "grad_norm": 1.295130729675293,
      "learning_rate": 3.9283387622149835e-05,
      "loss": 5.1747,
      "step": 330
    },
    {
      "epoch": 1.1075794621026895,
      "grad_norm": 1.406437635421753,
      "learning_rate": 3.895765472312703e-05,
      "loss": 5.3597,
      "step": 340
    },
    {
      "epoch": 1.1401792991035045,
      "grad_norm": 1.4687511920928955,
      "learning_rate": 3.863192182410424e-05,
      "loss": 5.2431,
      "step": 350
    },
    {
      "epoch": 1.1727791361043194,
      "grad_norm": 1.7459346055984497,
      "learning_rate": 3.830618892508144e-05,
      "loss": 5.2581,
      "step": 360
    },
    {
      "epoch": 1.2053789731051345,
      "grad_norm": 1.5150882005691528,
      "learning_rate": 3.7980456026058636e-05,
      "loss": 5.2601,
      "step": 370
    },
    {
      "epoch": 1.2379788101059495,
      "grad_norm": 1.403608798980713,
      "learning_rate": 3.765472312703583e-05,
      "loss": 5.1278,
      "step": 380
    },
    {
      "epoch": 1.2705786471067646,
      "grad_norm": 1.7031863927841187,
      "learning_rate": 3.732899022801303e-05,
      "loss": 5.1732,
      "step": 390
    },
    {
      "epoch": 1.3031784841075795,
      "grad_norm": 1.7674099206924438,
      "learning_rate": 3.7003257328990234e-05,
      "loss": 5.3195,
      "step": 400
    },
    {
      "epoch": 1.3357783211083945,
      "grad_norm": 1.6171581745147705,
      "learning_rate": 3.667752442996743e-05,
      "loss": 5.0874,
      "step": 410
    },
    {
      "epoch": 1.3683781581092094,
      "grad_norm": 1.4554940462112427,
      "learning_rate": 3.635179153094463e-05,
      "loss": 5.3694,
      "step": 420
    },
    {
      "epoch": 1.4009779951100245,
      "grad_norm": 1.6299272775650024,
      "learning_rate": 3.6026058631921824e-05,
      "loss": 5.0819,
      "step": 430
    },
    {
      "epoch": 1.4335778321108394,
      "grad_norm": 1.5950878858566284,
      "learning_rate": 3.570032573289903e-05,
      "loss": 5.163,
      "step": 440
    },
    {
      "epoch": 1.4661776691116544,
      "grad_norm": 1.6684515476226807,
      "learning_rate": 3.5374592833876225e-05,
      "loss": 5.3008,
      "step": 450
    },
    {
      "epoch": 1.4987775061124695,
      "grad_norm": 1.597968578338623,
      "learning_rate": 3.504885993485342e-05,
      "loss": 5.2997,
      "step": 460
    },
    {
      "epoch": 1.5313773431132844,
      "grad_norm": 1.5988012552261353,
      "learning_rate": 3.472312703583062e-05,
      "loss": 5.063,
      "step": 470
    },
    {
      "epoch": 1.5639771801140996,
      "grad_norm": 1.80494225025177,
      "learning_rate": 3.439739413680782e-05,
      "loss": 5.0356,
      "step": 480
    },
    {
      "epoch": 1.5965770171149143,
      "grad_norm": 1.5384434461593628,
      "learning_rate": 3.407166123778502e-05,
      "loss": 5.1169,
      "step": 490
    },
    {
      "epoch": 1.6291768541157294,
      "grad_norm": 1.6038174629211426,
      "learning_rate": 3.3745928338762216e-05,
      "loss": 5.2615,
      "step": 500
    },
    {
      "epoch": 1.6617766911165446,
      "grad_norm": 1.5681452751159668,
      "learning_rate": 3.342019543973941e-05,
      "loss": 5.1966,
      "step": 510
    },
    {
      "epoch": 1.6943765281173593,
      "grad_norm": 1.4932509660720825,
      "learning_rate": 3.309446254071662e-05,
      "loss": 4.8926,
      "step": 520
    },
    {
      "epoch": 1.7269763651181744,
      "grad_norm": 2.0387141704559326,
      "learning_rate": 3.2768729641693814e-05,
      "loss": 5.204,
      "step": 530
    },
    {
      "epoch": 1.7595762021189894,
      "grad_norm": 1.673019528388977,
      "learning_rate": 3.244299674267101e-05,
      "loss": 5.1109,
      "step": 540
    },
    {
      "epoch": 1.7921760391198043,
      "grad_norm": 1.7725902795791626,
      "learning_rate": 3.211726384364821e-05,
      "loss": 5.075,
      "step": 550
    },
    {
      "epoch": 1.8247758761206194,
      "grad_norm": 1.471029281616211,
      "learning_rate": 3.179153094462541e-05,
      "loss": 5.0804,
      "step": 560
    },
    {
      "epoch": 1.8573757131214343,
      "grad_norm": 1.719211220741272,
      "learning_rate": 3.146579804560261e-05,
      "loss": 5.1358,
      "step": 570
    },
    {
      "epoch": 1.8899755501222493,
      "grad_norm": 1.654223918914795,
      "learning_rate": 3.1140065146579805e-05,
      "loss": 5.0354,
      "step": 580
    },
    {
      "epoch": 1.9225753871230644,
      "grad_norm": 1.5948837995529175,
      "learning_rate": 3.0814332247557e-05,
      "loss": 4.9936,
      "step": 590
    },
    {
      "epoch": 1.9551752241238793,
      "grad_norm": 1.5865886211395264,
      "learning_rate": 3.0488599348534202e-05,
      "loss": 5.0837,
      "step": 600
    },
    {
      "epoch": 1.9877750611246943,
      "grad_norm": 1.5910431146621704,
      "learning_rate": 3.0162866449511402e-05,
      "loss": 5.2532,
      "step": 610
    },
    {
      "epoch": 2.019559902200489,
      "grad_norm": 1.6349103450775146,
      "learning_rate": 2.98371335504886e-05,
      "loss": 4.7882,
      "step": 620
    },
    {
      "epoch": 2.052159739201304,
      "grad_norm": 1.9672294855117798,
      "learning_rate": 2.95114006514658e-05,
      "loss": 4.8458,
      "step": 630
    },
    {
      "epoch": 2.084759576202119,
      "grad_norm": 1.7893781661987305,
      "learning_rate": 2.9185667752442996e-05,
      "loss": 4.9156,
      "step": 640
    },
    {
      "epoch": 2.117359413202934,
      "grad_norm": 1.8613340854644775,
      "learning_rate": 2.8859934853420197e-05,
      "loss": 4.8618,
      "step": 650
    },
    {
      "epoch": 2.149959250203749,
      "grad_norm": 1.7636836767196655,
      "learning_rate": 2.8534201954397394e-05,
      "loss": 4.9746,
      "step": 660
    },
    {
      "epoch": 2.182559087204564,
      "grad_norm": 1.702432632446289,
      "learning_rate": 2.8208469055374594e-05,
      "loss": 4.7934,
      "step": 670
    },
    {
      "epoch": 2.215158924205379,
      "grad_norm": 1.7502435445785522,
      "learning_rate": 2.788273615635179e-05,
      "loss": 4.7664,
      "step": 680
    },
    {
      "epoch": 2.247758761206194,
      "grad_norm": 1.8319793939590454,
      "learning_rate": 2.755700325732899e-05,
      "loss": 4.8414,
      "step": 690
    },
    {
      "epoch": 2.280358598207009,
      "grad_norm": 1.9470432996749878,
      "learning_rate": 2.7231270358306188e-05,
      "loss": 4.7593,
      "step": 700
    },
    {
      "epoch": 2.312958435207824,
      "grad_norm": 1.9225032329559326,
      "learning_rate": 2.6905537459283385e-05,
      "loss": 4.8082,
      "step": 710
    },
    {
      "epoch": 2.3455582722086388,
      "grad_norm": 2.0829355716705322,
      "learning_rate": 2.6579804560260585e-05,
      "loss": 4.8547,
      "step": 720
    },
    {
      "epoch": 2.378158109209454,
      "grad_norm": 2.026258707046509,
      "learning_rate": 2.6254071661237782e-05,
      "loss": 4.7898,
      "step": 730
    },
    {
      "epoch": 2.410757946210269,
      "grad_norm": 2.0470588207244873,
      "learning_rate": 2.592833876221499e-05,
      "loss": 4.8177,
      "step": 740
    },
    {
      "epoch": 2.4433577832110838,
      "grad_norm": 1.9404220581054688,
      "learning_rate": 2.5602605863192186e-05,
      "loss": 4.6707,
      "step": 750
    },
    {
      "epoch": 2.475957620211899,
      "grad_norm": 1.9422565698623657,
      "learning_rate": 2.5276872964169386e-05,
      "loss": 4.644,
      "step": 760
    },
    {
      "epoch": 2.508557457212714,
      "grad_norm": 2.146972894668579,
      "learning_rate": 2.495114006514658e-05,
      "loss": 4.6778,
      "step": 770
    },
    {
      "epoch": 2.541157294213529,
      "grad_norm": 1.8498471975326538,
      "learning_rate": 2.462540716612378e-05,
      "loss": 4.704,
      "step": 780
    },
    {
      "epoch": 2.573757131214344,
      "grad_norm": 1.889958143234253,
      "learning_rate": 2.4299674267100977e-05,
      "loss": 4.5985,
      "step": 790
    },
    {
      "epoch": 2.606356968215159,
      "grad_norm": 2.140632152557373,
      "learning_rate": 2.3973941368078177e-05,
      "loss": 4.6874,
      "step": 800
    },
    {
      "epoch": 2.6389568052159738,
      "grad_norm": 2.2482054233551025,
      "learning_rate": 2.3648208469055374e-05,
      "loss": 4.9243,
      "step": 810
    },
    {
      "epoch": 2.671556642216789,
      "grad_norm": 1.929128646850586,
      "learning_rate": 2.3322475570032574e-05,
      "loss": 4.8639,
      "step": 820
    },
    {
      "epoch": 2.704156479217604,
      "grad_norm": 2.3225066661834717,
      "learning_rate": 2.2996742671009775e-05,
      "loss": 4.6672,
      "step": 830
    },
    {
      "epoch": 2.7367563162184187,
      "grad_norm": 1.9984192848205566,
      "learning_rate": 2.267100977198697e-05,
      "loss": 4.7739,
      "step": 840
    },
    {
      "epoch": 2.769356153219234,
      "grad_norm": 1.960925817489624,
      "learning_rate": 2.2345276872964172e-05,
      "loss": 4.7928,
      "step": 850
    },
    {
      "epoch": 2.801955990220049,
      "grad_norm": 2.279360055923462,
      "learning_rate": 2.201954397394137e-05,
      "loss": 4.9368,
      "step": 860
    },
    {
      "epoch": 2.8345558272208637,
      "grad_norm": 2.1262946128845215,
      "learning_rate": 2.169381107491857e-05,
      "loss": 4.8286,
      "step": 870
    },
    {
      "epoch": 2.867155664221679,
      "grad_norm": 2.0853919982910156,
      "learning_rate": 2.1368078175895766e-05,
      "loss": 4.8524,
      "step": 880
    },
    {
      "epoch": 2.899755501222494,
      "grad_norm": 2.0597429275512695,
      "learning_rate": 2.1042345276872966e-05,
      "loss": 4.7678,
      "step": 890
    },
    {
      "epoch": 2.9323553382233087,
      "grad_norm": 2.4644925594329834,
      "learning_rate": 2.0716612377850163e-05,
      "loss": 4.7785,
      "step": 900
    },
    {
      "epoch": 2.964955175224124,
      "grad_norm": 2.0731208324432373,
      "learning_rate": 2.0390879478827363e-05,
      "loss": 4.8706,
      "step": 910
    },
    {
      "epoch": 2.997555012224939,
      "grad_norm": 2.102787971496582,
      "learning_rate": 2.006514657980456e-05,
      "loss": 4.7308,
      "step": 920
    },
    {
      "epoch": 3.0293398533007334,
      "grad_norm": 2.009148597717285,
      "learning_rate": 1.973941368078176e-05,
      "loss": 4.5728,
      "step": 930
    },
    {
      "epoch": 3.0619396903015486,
      "grad_norm": 2.0409862995147705,
      "learning_rate": 1.9413680781758957e-05,
      "loss": 4.6042,
      "step": 940
    },
    {
      "epoch": 3.0945395273023637,
      "grad_norm": 2.0259461402893066,
      "learning_rate": 1.9087947882736158e-05,
      "loss": 4.578,
      "step": 950
    },
    {
      "epoch": 3.1271393643031784,
      "grad_norm": 2.4443397521972656,
      "learning_rate": 1.8762214983713355e-05,
      "loss": 4.4703,
      "step": 960
    },
    {
      "epoch": 3.1597392013039935,
      "grad_norm": 2.1314215660095215,
      "learning_rate": 1.8436482084690555e-05,
      "loss": 4.5661,
      "step": 970
    },
    {
      "epoch": 3.1923390383048087,
      "grad_norm": 2.349651575088501,
      "learning_rate": 1.8110749185667752e-05,
      "loss": 4.5898,
      "step": 980
    },
    {
      "epoch": 3.2249388753056234,
      "grad_norm": 2.2346479892730713,
      "learning_rate": 1.7785016286644952e-05,
      "loss": 4.4271,
      "step": 990
    },
    {
      "epoch": 3.2575387123064385,
      "grad_norm": 2.3576178550720215,
      "learning_rate": 1.745928338762215e-05,
      "loss": 4.5897,
      "step": 1000
    },
    {
      "epoch": 3.2901385493072537,
      "grad_norm": 2.2631664276123047,
      "learning_rate": 1.713355048859935e-05,
      "loss": 4.4351,
      "step": 1010
    },
    {
      "epoch": 3.3227383863080684,
      "grad_norm": 2.050614356994629,
      "learning_rate": 1.680781758957655e-05,
      "loss": 4.4988,
      "step": 1020
    },
    {
      "epoch": 3.3553382233088835,
      "grad_norm": 2.2464025020599365,
      "learning_rate": 1.6482084690553746e-05,
      "loss": 4.5326,
      "step": 1030
    },
    {
      "epoch": 3.3879380603096987,
      "grad_norm": 2.1007516384124756,
      "learning_rate": 1.6156351791530947e-05,
      "loss": 4.3373,
      "step": 1040
    },
    {
      "epoch": 3.4205378973105134,
      "grad_norm": 2.4122960567474365,
      "learning_rate": 1.5830618892508144e-05,
      "loss": 4.4778,
      "step": 1050
    },
    {
      "epoch": 3.4531377343113285,
      "grad_norm": 2.2852895259857178,
      "learning_rate": 1.5504885993485344e-05,
      "loss": 4.5663,
      "step": 1060
    },
    {
      "epoch": 3.4857375713121437,
      "grad_norm": 2.2754862308502197,
      "learning_rate": 1.5179153094462542e-05,
      "loss": 4.461,
      "step": 1070
    },
    {
      "epoch": 3.5183374083129584,
      "grad_norm": 2.2863850593566895,
      "learning_rate": 1.4853420195439741e-05,
      "loss": 4.5258,
      "step": 1080
    },
    {
      "epoch": 3.5509372453137735,
      "grad_norm": 2.299619436264038,
      "learning_rate": 1.452768729641694e-05,
      "loss": 4.6066,
      "step": 1090
    },
    {
      "epoch": 3.5835370823145887,
      "grad_norm": 2.472318172454834,
      "learning_rate": 1.4201954397394138e-05,
      "loss": 4.6085,
      "step": 1100
    },
    {
      "epoch": 3.6161369193154034,
      "grad_norm": 2.32785701751709,
      "learning_rate": 1.3876221498371337e-05,
      "loss": 4.5935,
      "step": 1110
    },
    {
      "epoch": 3.6487367563162185,
      "grad_norm": 2.405393123626709,
      "learning_rate": 1.3550488599348535e-05,
      "loss": 4.511,
      "step": 1120
    },
    {
      "epoch": 3.6813365933170337,
      "grad_norm": 2.4606173038482666,
      "learning_rate": 1.3224755700325734e-05,
      "loss": 4.4723,
      "step": 1130
    },
    {
      "epoch": 3.7139364303178484,
      "grad_norm": 2.473886013031006,
      "learning_rate": 1.2899022801302933e-05,
      "loss": 4.6675,
      "step": 1140
    },
    {
      "epoch": 3.7465362673186635,
      "grad_norm": 2.260284423828125,
      "learning_rate": 1.2573289902280131e-05,
      "loss": 4.5072,
      "step": 1150
    },
    {
      "epoch": 3.7791361043194787,
      "grad_norm": 2.3545916080474854,
      "learning_rate": 1.224755700325733e-05,
      "loss": 4.4204,
      "step": 1160
    },
    {
      "epoch": 3.8117359413202934,
      "grad_norm": 2.3132688999176025,
      "learning_rate": 1.1921824104234528e-05,
      "loss": 4.5894,
      "step": 1170
    },
    {
      "epoch": 3.8443357783211085,
      "grad_norm": 2.303913116455078,
      "learning_rate": 1.1596091205211727e-05,
      "loss": 4.4682,
      "step": 1180
    },
    {
      "epoch": 3.8769356153219237,
      "grad_norm": 2.519808292388916,
      "learning_rate": 1.1270358306188925e-05,
      "loss": 4.5233,
      "step": 1190
    },
    {
      "epoch": 3.9095354523227384,
      "grad_norm": 2.2594923973083496,
      "learning_rate": 1.0944625407166124e-05,
      "loss": 4.6247,
      "step": 1200
    },
    {
      "epoch": 3.9421352893235535,
      "grad_norm": 2.563668727874756,
      "learning_rate": 1.0618892508143323e-05,
      "loss": 4.4878,
      "step": 1210
    },
    {
      "epoch": 3.9747351263243687,
      "grad_norm": 2.4647445678710938,
      "learning_rate": 1.0293159609120521e-05,
      "loss": 4.1218,
      "step": 1220
    },
    {
      "epoch": 4.006519967400163,
      "grad_norm": 2.308682680130005,
      "learning_rate": 9.96742671009772e-06,
      "loss": 4.5353,
      "step": 1230
    },
    {
      "epoch": 4.039119804400978,
      "grad_norm": 2.2981116771698,
      "learning_rate": 9.641693811074918e-06,
      "loss": 4.2248,
      "step": 1240
    },
    {
      "epoch": 4.071719641401793,
      "grad_norm": 2.425006866455078,
      "learning_rate": 9.315960912052117e-06,
      "loss": 4.4726,
      "step": 1250
    },
    {
      "epoch": 4.104319478402608,
      "grad_norm": 2.4834482669830322,
      "learning_rate": 8.990228013029317e-06,
      "loss": 4.307,
      "step": 1260
    },
    {
      "epoch": 4.136919315403423,
      "grad_norm": 2.2821292877197266,
      "learning_rate": 8.664495114006516e-06,
      "loss": 4.3141,
      "step": 1270
    },
    {
      "epoch": 4.169519152404238,
      "grad_norm": 2.584432363510132,
      "learning_rate": 8.338762214983714e-06,
      "loss": 4.3982,
      "step": 1280
    },
    {
      "epoch": 4.202118989405053,
      "grad_norm": 2.4492337703704834,
      "learning_rate": 8.013029315960913e-06,
      "loss": 4.365,
      "step": 1290
    },
    {
      "epoch": 4.234718826405868,
      "grad_norm": 2.438023328781128,
      "learning_rate": 7.687296416938112e-06,
      "loss": 4.2438,
      "step": 1300
    },
    {
      "epoch": 4.267318663406683,
      "grad_norm": 2.1884524822235107,
      "learning_rate": 7.361563517915309e-06,
      "loss": 4.2758,
      "step": 1310
    },
    {
      "epoch": 4.299918500407498,
      "grad_norm": 2.4056551456451416,
      "learning_rate": 7.035830618892508e-06,
      "loss": 4.3321,
      "step": 1320
    },
    {
      "epoch": 4.332518337408313,
      "grad_norm": 2.423060178756714,
      "learning_rate": 6.7100977198697066e-06,
      "loss": 4.3836,
      "step": 1330
    },
    {
      "epoch": 4.365118174409128,
      "grad_norm": 2.641853094100952,
      "learning_rate": 6.384364820846905e-06,
      "loss": 4.4559,
      "step": 1340
    },
    {
      "epoch": 4.397718011409943,
      "grad_norm": 2.503373384475708,
      "learning_rate": 6.0586319218241046e-06,
      "loss": 4.2198,
      "step": 1350
    },
    {
      "epoch": 4.430317848410758,
      "grad_norm": 2.525970935821533,
      "learning_rate": 5.732899022801303e-06,
      "loss": 4.3812,
      "step": 1360
    },
    {
      "epoch": 4.462917685411573,
      "grad_norm": 2.409959554672241,
      "learning_rate": 5.407166123778502e-06,
      "loss": 4.2257,
      "step": 1370
    },
    {
      "epoch": 4.495517522412388,
      "grad_norm": 2.4199256896972656,
      "learning_rate": 5.081433224755701e-06,
      "loss": 4.2558,
      "step": 1380
    },
    {
      "epoch": 4.528117359413203,
      "grad_norm": 2.363055467605591,
      "learning_rate": 4.7557003257329e-06,
      "loss": 4.351,
      "step": 1390
    },
    {
      "epoch": 4.560717196414018,
      "grad_norm": 2.418140411376953,
      "learning_rate": 4.429967426710098e-06,
      "loss": 4.3557,
      "step": 1400
    },
    {
      "epoch": 4.5933170334148326,
      "grad_norm": 2.4495174884796143,
      "learning_rate": 4.104234527687296e-06,
      "loss": 4.3891,
      "step": 1410
    },
    {
      "epoch": 4.625916870415648,
      "grad_norm": 2.5640299320220947,
      "learning_rate": 3.778501628664495e-06,
      "loss": 4.2258,
      "step": 1420
    },
    {
      "epoch": 4.658516707416463,
      "grad_norm": 2.437870979309082,
      "learning_rate": 3.452768729641694e-06,
      "loss": 4.3432,
      "step": 1430
    },
    {
      "epoch": 4.6911165444172775,
      "grad_norm": 2.5305392742156982,
      "learning_rate": 3.1270358306188927e-06,
      "loss": 4.3282,
      "step": 1440
    },
    {
      "epoch": 4.723716381418093,
      "grad_norm": 2.6606602668762207,
      "learning_rate": 2.8013029315960913e-06,
      "loss": 4.4039,
      "step": 1450
    },
    {
      "epoch": 4.756316218418908,
      "grad_norm": 2.7058300971984863,
      "learning_rate": 2.47557003257329e-06,
      "loss": 4.3534,
      "step": 1460
    },
    {
      "epoch": 4.7889160554197225,
      "grad_norm": 2.470324754714966,
      "learning_rate": 2.149837133550489e-06,
      "loss": 4.1068,
      "step": 1470
    },
    {
      "epoch": 4.821515892420538,
      "grad_norm": 2.534902811050415,
      "learning_rate": 1.8241042345276873e-06,
      "loss": 4.2859,
      "step": 1480
    },
    {
      "epoch": 4.854115729421353,
      "grad_norm": 2.4135072231292725,
      "learning_rate": 1.498371335504886e-06,
      "loss": 4.316,
      "step": 1490
    },
    {
      "epoch": 4.8867155664221675,
      "grad_norm": 2.5558457374572754,
      "learning_rate": 1.1726384364820847e-06,
      "loss": 4.3202,
      "step": 1500
    },
    {
      "epoch": 4.919315403422983,
      "grad_norm": 2.3487424850463867,
      "learning_rate": 8.469055374592835e-07,
      "loss": 4.2449,
      "step": 1510
    },
    {
      "epoch": 4.951915240423798,
      "grad_norm": 2.6684863567352295,
      "learning_rate": 5.211726384364822e-07,
      "loss": 4.3538,
      "step": 1520
    },
    {
      "epoch": 4.9845150774246125,
      "grad_norm": 2.353093147277832,
      "learning_rate": 1.954397394136808e-07,
      "loss": 4.3969,
      "step": 1530
    }
  ],
  "logging_steps": 10,
  "max_steps": 1535,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1831569806131200.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
